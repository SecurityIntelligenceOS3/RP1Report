%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{ragged2e}
\usepackage[none]{hyphenat}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage[title,titletoc,toc]{appendix}
\usepackage{kantlipsum}
\usepackage{spverbatim}
\usepackage{caption3}


\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 
\center

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

\includegraphics{images/uva.jpeg}\\[0.5cm]% Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------
\textsc{\Large System and Network Engineering, MSc}\\[0.5cm] 
\textsc { \large Research Project 1}\\[0.4cm] % Title of your document

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------
\HRule \\[0.4cm]
{ \huge \bfseries Security Intelligence Data Mining}\\[0.4cm] % Title of your document
{ \large \bfseries Research Paper}\\[0.4cm] % Title of your document
\HRule \\[0.4cm]




%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------


\large Diana Rusu\\
{\bfseries Diana.Rusu@os3.nl}\\[0.5cm]
\large Nikolaos Petros Triantafyllidis\\
\bfseries Nikolaos.Triantafyllidis@os3.nl\\[2cm]

{\large \today} 

\end{titlepage}
\newpage
\section*{Abstract}
\newpage
\section*{Acknowledgement}

We would like to express our gratitude and appreciation for all support, expert knowledge and guidelines during this research project. It has been a great opportunity for us to explore the real feel of working with a successful company. 
\\
\\
Who should we mention? 
\begin{itemize}
\item{dhr. prof. dr. ir. C.T.A.M. (Cees) de Laat - for the project proposals}
\item{Delloite NL specifically to our first supervisor Henri Hambartsumyan - for giving us access inside their company and offices, for all the experts and employees that we had the chance to meet}
 \begin{itemize}
 \item{Henri Hambartsumyan - who proposed Security Intelligence Data Mining}
 \item{Kremers, Joost  - our main supervisor for guideline during the research }
 \item{Niels - Cyber Threat Intelligence project}
 \item{Ari Davies - proposed making the plugins also more depth in CTI }
 \item{Gijs Hollestelle - presented the actual CTI platform and the portal to it, proposed phishtank and cleared out the purpose for this project(making some agents for IR system) }
 \end{itemize}
\item{dhr. dr. C.P.J. (Karst) Koymans}
\end{itemize}


 
\newpage

\tableofcontents
\newpage

\section*{Introduction}
\addcontentsline{toc}{section}{Introduction}
\parbox{\linewidth}{
With the increasing number of cyber-attacks and the growth of computer crime worldwide, it becomes apparent that IT security is a major concern and crucial survival factor for large companies, organisations and institutions of any sort. Security Operations departments working to ensure confidentiality, integrity and availability for the system infrastructure of their organisation, invest huge  parts \cite{cyber} of their time and effort in detecting threats in real time. A very valuable source of security intelligence, vital to cyber-risk assessment, is information mined from data posted on public sites such as "pastebins" or social networks. However, this is a very cumbersome task due to the lack of Natural Language Processing capabilities in most of the existing tools. Moreover, as recent events have showcased \cite{sony}, several threats arise from governments and criminal associations originating from countries whose languages use non-latin scripts (Chinese, Russian, Korean, etc.). It is, hence, important to have data mining tools that provide support for such alphabets and languages, since a lot of a security intelligence can be discovered in such texts. The main goals of this research project will be to explore the various public data and detect the most appropriate among them. Moreover, numerous current data analytics techniques as well as their application on security related issues will be assessed. Lastly the above knowledge will be applied on the implementation of a simple system that will work as a proof-of-concept and help determine the technical feasibility, storage requirements and operational cost of such a system. This project was proposed by and will carried out in co-operation with Deloitte NL. 
}
\newpage
\section{Research Questions}
This topic is admittedly very open but it can be narrowed down to several specific research questions some of which we will try to answer to some extent. The main question that we will be trying to answer is the following:\\[0.1cm]

\noindent
\textbf{How can we effectively use public sources to obtain real time information about security incidents?}\\[0.1cm]

\noindent
This question can be analysed into more specific parts that cover the topic to some extent, as follows:

\begin{enumerate}
	\item How can the raw data be effectively collected from the public sources? 
	\begin{itemize}
		\item How can we effectively detect the reliable sources?
		\item What search terms can we deploy during the retrieval phase?
		\item How can the unstructured data be pre-processed? 
	\end{itemize}
	\item How can the data be analysed in respect to security operations?
	\begin{itemize}
		\item How can we apply current Data Mining and Analytics techniques on Security issues?
		\item How can we derive the risk assessment model from the above?
		\item How can we apply the model on new data?
	\end{itemize}
	\item	How can the collected knowledge be applied on a system implementation?
	\begin{itemize}
		\item What is a reliable and extensible System Architecture that can be designed?
		\item What are the computational and storage requirements of such a system?
		\item What extensions can be proposed for that system?
	\end{itemize}
\end{enumerate}

The proposed system extensions can spawn further research questions, namely on the topics of presenting the analysed data, reacting to the real time events and finally assessing the situations that arise and providing feedback to the system.


\section{Related work}
There is a lot of literature around the field of Data Mining and more recently Web Mining. The most prominent and recent case is the book 'Mining The Social Web' by M.Russel \cite{socialweb} that deals with exploring and mining information from social websites (e.g., Facebook, Twitter, LinkedIn, Google+, GitHub, etc.). There are also several academic papers and books that deal with applying Data Mining to System Security. One example is a system proposed by the university of Minnesota, called MINDS, that employs various Data Mining in Intrusion Detection. The system is described in their paper 'Data Mining for Cyber Security' \cite{minds}.  Another example is a system proposed by the Dutch company Sentient in co-operation with the Amsterdam Police Force \cite{police} aiming to provide Data Analytics operations automation while on the same time minimising the technical expertise needed by the system user. 

\section{Experimental installation requirements}
\parbox{\linewidth}{
Most of the work is going to be carried out on end workstations (desktops, laptops, etc.). Depending on the amount of data collected, additional computational or storage resources might be required. In that case our assigned OS3 servers can be used. As for software requirements, there are several open software tools (Database Systems, IR Systems, Web Crawlers, etc.) that can help us carry out the work. For each component of the system that has to be implemented manually the appropriate programming languages as well as libraries are going to be selected. One example is the very strong Python NLP-toolkit.\cite{nltk} }

\newpage
\section{Information Retrieval System}
\parbox{\linewidth}{
\paragraph{}
This chapter describes a system for collecting and mining information from public sources. First subsection will define the relevant websites that provide interest in detecting threats, malwares and phishing sites. For each specific source, next subsections will investigate API functionality and steps in implementing crawling part. 
}
\subsection{Defining reliable sources}
\parbox{\linewidth}{
\paragraph{}
Discovering real time threats with a plausible margin of error, became a challenging task as Internet provides an enormous database with random messages and information. Among these sites, significant information related to Security Intelligence can be found. Therefore it is important to define reliable sources that being monitored can provide significant valuable data for detecting future or present possible attacks. Special attention has to be provided in establishing which are the optimum sources that will provide the seeking information. 
\paragraph{}
Which sources can be considered as reliable? In order to answer this question inside research has been given by experts. From a company perspective it is important to have an overview over all general possible attacks or client specific. Among all the possible feed sources, four are considered at this moment mostly valuable. Other sources that provide RSS feeds or IRC channels will not be investigated due to time limitations and company requirements. This research will primarily focus on the following sites: Phishtank, Pastebin, Twitter and Reddit. Mainly, these are considered fundamental for the reason that past activity revealed vulnerabilities found through posted messages.  Furthermore, mentioned sources will be analysed in particular and be judged by the relevance of the subjects involved in discussion. 
}
   

%\subsection{Specific source analyse}
\subsubsection*{Phishtank}
\addcontentsline{toc}{subsection}{Phishtank}
\subsubsection{API Investigation/Inspection}
\subsubsection{Implementation}
\subsubsection*{General "Pastebins" information}
\addcontentsline{toc}{subsection}{Pastebins}
\paragraph{}
"Pastebins" websites \cite{fpaste} \cite{pastebin} allow everyone with or without registration to share real time text or code snippets. It attracted many users during past years including cybercriminals like malware writers\cite{pastebin-magazine}. The enormous flow of information include from database dumps containing e-mails and passwords to harmful backdoor programs. At a deeper examination of these public pasted messages possible future attacks can be determined. From all possible "pastebins" sources (e.g fpaste.org, paste2.org, pastie.org\cite{pastebin-pastie}, paste.ubuntu.org.cn \cite{pastebin-ubuntu} etc.) pastebin.com \cite{pastebin} will be further inspected and monitored.

\subsubsection{API Investigation}
\parbox{\linewidth}{
\paragraph{}
Pastebin API offers different options for developers, more specifically, creating new pastes, listing trending pastes and pastes from a particular user. This extensions can be used once one obtains the unique Developer API Key. 
Unfortunately, Pastebin API \cite{pastebin} does not offer complete information retrieval option for listing all new messages that arrive from all users, registered or not. This makes the crawling module challenging but not impossible. A method will be introduced to overcome this limitation. Search method for retrieving information using keywords uses third party software namely, "Google Custom Search". Therefore, if one desires to obtain full database it would have to use Google's API. Free users account will be limited to 1000 pastes per day. 
}

\subsubsection{Implementation}

To overpass API extension limitation and mine all new pastes that arrive in real time a web crawler module will be implemented. Python offers an open source framework called "scrapy" \cite{scrapy} for building an extensible webcrawler. Pastebin's archive page contains last pastes posted in the last 10 minutes at least. We will crawl and fetch this page each 10 minutes and store data into mongoDB database. Not all data from the page provides interest this is why we will scrape only information that is relevant for further examination. This includes the url, paste, the date it was posted and the number of unique views. By inspecting the elements required from the DOM tree, we then can parse this nodes and extract only the text. Once all this data has been gathered, it will be dumped in the database.

 
 

\subsubsection*{Twitter}
\addcontentsline{toc}{subsection}{Twitter}
\subsubsection{API Investigation}
\subsubsection{Implementation}
\subsubsection*{Reddit}
\addcontentsline{toc}{subsection}{Reddit}
Reddit community comprises of social websites and news sites. It consists of over 5000 channels called "subreddits" which belong to different categories. This research will mine Reddit \cite{reddit} data from the following branches:
\begin{itemize}
\item /r/blackhat \cite{red-blackhat}
\item r/malware \cite{r.malware}
\item r/netsec \cite{r.netsec}
\item r/pwned \cite{r.pwned}
\item /r/vrd/ \cite{r.rvd}

\end{itemize}

\subsubsection{API Investigation}
Reddit API\cite{reddit} provides a rich list of methods and it is easy to use. For listing new, top, controversial etc. messages is sufficient to add "/" to the subreddit one wants to obtain information. Endpoints can be used to filter and sort posts namely, after/before,limit, count and show \cite{reddit}. This parameters will be later used in retrieving lists of messages in json format. 

\subsubsection{Implementation}

All ingredients for implementing a retrieval agent for Reddit are supplied by its API. The only part we need to do is to filter and obtain data that further investigated and processed can help defining the relevance of this information. First a connection needs to be opened to the the targeted subreddit, in this case /r/blackhat. 
\begin{spverbatim}
response = urllib2.urlopen('http://www.reddit.com/r/blackhat/new
			.json?sort=new&limit=100')
\end{spverbatim}
\hfill \break
In response variable we will get a file in json format, from which we will store the title associated to the url,number of comments and score. Python provides an open json library for processing this type of files. All scraped Reddit data will be stored in mongoDB database.

%\figurename{ Scraped Reddit data stored in MongoDB }
\begin{figure}[h!]
\begin{spverbatim}
> db.redditData.find()
{ "_id" : ObjectId("54be5613d177b4501eedaec0"), "title" : "XSS (Cross Site Scripting): Everything you need to know (Tutorial)", "url" : "http://h.acker.in/tutorials/xss/", "num_comments" : 4, "score" : 37, "ups" : 37 }
{ "_id" : ObjectId("54be5613d177b4501eedaec1"), "title" : "2014 Top Security Tools", "url" : "http://www.digitalmunition.me/?p=3639", "num_comments" : 4, "score" : 30, "ups" : 30 }
{ "_id" : ObjectId("54be5613d177b4501eedaec2"), "title" : "KeySweeper — Arduino-based Keylogger for Wireless Keyboards", "url" : "http://www.digitalmunition.me/?p=3629", "num_comments" : 1, "score" : 43, "ups" : 43 }
{ "_id" : ObjectId("54be5613d177b4501eedaec3"), "title" : "Amazing Tricks - How to spy on Viber Messages?", "url" : "http://smstrackers.com/how-to-track-viber-messages/", "num_comments" : 1, "score" : 0, "ups" : 0 }
\end{spverbatim}
\captionsetup{font=small}
\caption{List of documents from mongoDB. Each document contains a unique objectID, url, title, number of comments and the associated score  }
\end{figure}

\newpage
\section{Define Security Intelligence related Keywords}

To determine what keywords are relevant for retrieving specific information from database, threat types will be outlined. At this point depends on what category of possible attacks one wants to be alerted on. Assuming that our target are all possible general attacks the most common and popular will be depicted and a list with keywords, provided. 

\begin{itemize}
\item Denial-of-Service (DoS) (causes computer crush )
\item Trojan horse programs (can change configuration files, install different programs without one knowing it)
\begin{itemize}
\item social engineering
\item back door programs
\end{itemize}
\item Malicious spyware (spying on victims) 
\item Back door and remote administration programs ( can get full control and access on Windows : BackOrifice, Netbus, and SubSeven) 
\item Mobile code (Java/JavaScript/ActiveX)
\item Spoofing
\item Email borne viruses (viruses in attachments)
\item Cross-site scripting
\item Hidden file extension
\end{itemize}

There are many type of threats and the previous list can be expended.  

\section{Keyword Search Implementation}
As a first step we need to ensure that the data hosted in database does not contain duplicates and empty pastes, this will only increase storage and time in retrieving useful information.
\begin{spverbatim}
#remove duplicates
collection.ensure_index( [("paste",pymongo.ASCENDING),
 ("unique",True), ("dropDups",True )] )
#remove empty pastes
collection.remove({"paste":[]})
\end{spverbatim}
\hfill \break
Applying a regular expression containing the defined keywords only documents that obey this rule will be retrieved.
\begin{spverbatim}
%query = {"paste":{'$regex':'.*'+keyword[i]+'.*'}}
\end{spverbatim}
Moreover, the keywords will be counted to determine the importance of a document. At this stage the question might rise. Why we are not applying the TF-IDF alghorithm to determine the relevance and weight for the documents? 
Following this insight a general overview of how this algorithm works and the obtained results will be further discussed. 

TF-IDF \cite{tf-idf} stands for term frequency–inverse document frequency and it is used to statistically score the importance of a word in a document across/in multiple/collection of documents. A word will be considered important if it appears multiple times into a document. TF is calculated by counting how many times the specific word appears over the total number of words from that document and it will get a high score. But if it appears and in other documents it will not be as unique and will receive a lower score. Having this in mind we implemented and applied the algorithm  on the retrieved data but the results turned out to be not sufficient to grade the value of the documents.
\section{Data Analyse}
This section will sketch few methods that can be applied over the congregated information. All the unstructured data has to be processed and converted to "clean" structured information which includes natural language processing. For this step Python provides an open source library nltk \cite{nltk}. We have to dip into several phases to get a good transformation of  paste, mesages or titles. This include stop words removal, stemming and pos(part of speach) detection. 
At this moment there are various data mining techniques \cite{oracle-list} that include text processing. Investigating all this techniques is out of scope in this project, therefore a general analyse will be provided. 
\begin{itemize}
\item Classification
\item Anomaly Detection
\item Association Rule Detection
\item Clustering
\item Attribute Important 
\item etc.
\end{itemize}

In data mining step or knowledge discovery from database(KDD) it is important to find and extract patterns and knowledge from database not the data itself \cite{data-kdd}.  
 
\subsection{Defining data mining model}
\subsection{Implementation}

\newpage
\section{System Architecture}
\newpage
\section*{Conclusions}
\addcontentsline{toc}{section}{Conclusions}
Obtained results during this research from several sources, such as those that include social networking websites, "pastebin" and phishing clearly shows that monitoring them one can get information about the security intelligence. Moreover, analysing them and applying data mining techniques or searching by keywords in the database they can be filtered and from the thousands of messages collected only the "relevant" will be obtained.  Built system can be expanded and included in the CTI project.
\newpage
\section*{Implications}
\addcontentsline{toc}{section}{Implications}
\newpage
\section*{Further Research}
\addcontentsline{toc}{section}{Further Research}
\newpage
\section*{Ethical implications}
\addcontentsline{toc}{section}{Ethical implications}
The main part of this research comprises of exploring current techniques and their application on IT security, as well as the specification of a system that employs Data Mining techniques to collect security intelligence. In order for the models to be defined some amount of information will have to be gathered. This information will originate solely from public sources and will be mostly historical data. In the unlikely case that any previously unnoticed security issues are encountered they will be handled with discretion and communicated only towards the appropriate targets and only with the approval of the OS3 core team and Deloitte Digital. The collection or storage of personal data is not intended and any collected information will be discarded after the end of this project. The usage of shared computational and network infrastructure will only be used for the needs of this project and within the legal limits.
\newpage
\section*{Appendices}
\newpage

\begin{thebibliography}{99}
\bibitem{cyber}
	Anon, (2015). [online] US cybercrime: Rising risks, reduced readiness, PWC, Available at: \url{http://www.pwc.com/en_US/us/increasing-it-effectiveness/publications/assets/2014-us-state-of-cybercrime.pdf}
\bibitem{sony}
	 D. Sanger, N. Perlroth.  U.S. Said to Find North Korea Ordered Cyberattack on Sony. [online] Nytimes.com. \\Available at: \url{http://www.nytimes.com/2014/12/18/world/asia/us-links-north-korea-to-sony-hacking.html?_r=0}
\bibitem{socialweb}
  Russell, MA (2014). Mining the Social Web, O'Reily Media, USA
\bibitem{minds}
    V. Chandola et al. Data Mining for Cyber Security, Department of Computer Science, University of Minnesota, Springer, 2006
\bibitem{police}
   RCP van der Veer, H.T. Roo,  A. van der Zanden, Data mining for intelligence led policing, Sentient, Amsterdam Police Force, Amsterdam, The Netherlands, 2009
\bibitem{nltk}
Nltk.org, (2015). Natural Language Toolkit - NLTK 3.0 documentation. [online] Available at: \url{http://www.nltk.org/}

\bibitem{fpaste}
Fedora Project Pastebin Fpaste.org, (2015). New paste • Fedora Project Pastebin. [online] Available at:  
\url{http://fpaste.org }
\bibitem{pastebin}
Pastebin, (2015). Pastebin.com - \#1 paste tool since 2002!. [online] Available at:     \url{http://pastebin.com/}

\bibitem{pastebin-magazine}
Security Intelligence, (2015). Pastebin a Convenient Way for Cybercriminals to Remotely Host Malware. [online] Available at: \url{http://securityintelligence.com/news/pastebin-convenient-way-cybercriminals-remotely-host-malware/#.VMS25DX8vCI }
\bibitem{pastebin-pastie}
Pastie.org, (2015). New - Pastie. [online] Available at: \url{http://pastie.org} 
\bibitem{pastebin-ubuntu}
 Paste.ubuntu.org.cn, (2015). [online] Available at: \url{http://paste.ubuntu.org.cn/}
\bibitem{scrapy}
Scrapy.org, (2015). Scrapy | A Fast and Powerful Scraping and Web Crawling Framework. [online] Available at: \url{http://scrapy.org/} 
\bibitem{reddit}
Reddit.com, (2015). reddit: the front page of the internet. [online] Available at: \url{http://www.reddit.com/} 
\bibitem{red-blackhat}
Reddit, (2009). blackhat library: Documenting blackhat hacking techniques • /r/blackhat. [online] Available at: \url{https://www.reddit.com/r/blackhat}
\bibitem{r.malware}
Reddit, (2009). Malware Analysis \& Reports • /r/Malware. [online] Available at:   \url{http://www.reddit.com/r/malware}
\bibitem{r.netsec}
reddit, (2007). /r/netsec - Information Security News \& Discussion. [online] Available at: \url{https://www.reddit.com/r/netsec/} 
\bibitem{r.pwned}
Reddit, (2008). pwned • /r/pwned. [online] Available at: http://www.reddit.com/r/pwned 
\bibitem{r.rvd}
Reddit, (2012). Vulnerability Research and Development • /r/vrd. [online] Available at: \url{http://www.reddit.com/r/vrd/}
\bibitem{oracle-list}
Oracle.com, (2015). Oracle Data Mining Techniques and Algorithms. [online] Available at: \url{http://www.oracle.com/technetwork/database/enterprise-edition/odm-techniques-algorithms-097163.html}
\bibitem{tf-idf}
Tfidf.com, (2015). Tf-idf : A Single-Page Tutorial - Information Retrieval and Text Mining. [online] Available at: \url{http://www.tfidf.com/} 
\bibitem{data-kdd}
Han, Jiawei; Kamber, Micheline (2001). Data mining: concepts and techniques. Morgan Kaufmann. p. 5. ISBN 9781558604896. "Thus, data mining should have been more appropriately named "knowledge mining from data," which is unfortunately somewhat long"
\end{thebibliography}

\end{document}
