%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{ragged2e}
\usepackage[none]{hyphenat}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage[title,titletoc,toc]{appendix}
\usepackage{kantlipsum}
\usepackage{spverbatim}
\usepackage{caption3}
\usepackage{longtable}
% This is for adding extra subsections
\usepackage{titlesec}
\titleclass{\subsubsubsection}{straight}[\subsection]
\newcounter{subsubsubsection}[subsubsection]
\renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}
\renewcommand\theparagraph{\thesubsubsubsection.\arabic{paragraph}}
\renewcommand\thesubparagraph{\theparagraph.\arabic{subparagraph}}

\titleformat{\subsubsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
\titlespacing*{\subsubsubsection}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
  {3.25ex \@plus1ex \@minus.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\renewcommand\subparagraph{\@startsection{subparagraph}{6}{\parindent}
  {3.25ex \@plus1ex \@minus .2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\def\toclevel@subsubsubsection{4}
\def\toclevel@paragraph{5}
\def\toclevel@paragraph{6}
\def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
\def\l@paragraph{\@dottedtocline{5}{10em}{5em}}
\def\l@subparagraph{\@dottedtocline{6}{14em}{6em}}
\@addtoreset{subsubsubsection}{section}
\@addtoreset{subsubsubsection}{subsection}
\@addtoreset{paragraph}{subsubsubsection}
\makeatother
\setcounter{secnumdepth}{6}
\setcounter{tocdepth}{6}
%-----------------------------------------------%

\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 
\center

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

\includegraphics{images/uva.jpeg}\\[0.5cm]% Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------
\textsc{\Large System and Network Engineering, MSc}\\[0.5cm] 
\textsc { \large Research Project 1}\\[0.4cm] % Title of your document

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------
\HRule \\[0.4cm]
{ \huge \bfseries Security Intelligence Data Mining}\\[0.4cm] % Title of your document
{ \large \bfseries Research Proposal}\\[0.4cm] % Title of your document
\HRule \\[0.4cm]




%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------


\large Diana Rusu\\
{\bfseries Diana.Rusu@os3.nl}\\[0.5cm]
\large Nikolaos Petros Triantafyllidis\\
\bfseries Nikolaos.Triantafyllidis@os3.nl\\[2cm]

{\large \today} 

\end{titlepage}

\newpage
\section*{Abstract}
\newpage
\section*{Acknowledgement}

We would like to express our gratitude and appreciation for all support, expert knowledge and guidelines during this research project. It has been a great opportunity for us to explore the real feel of working with a successful company. 
\\
\\
Who should we mention? 
\begin{itemize}
\item{dhr. prof. dr. ir. C.T.A.M. (Cees) de Laat - for the project proposals}
\item{Delloite NL specifically to our first supervisor Henri Hambartsumyan - for giving us access inside their company and offices, for all the experts and employees that we had the chance to meet}
 \begin{itemize}
 \item{Henri Hambartsumyan - who proposed Security Intelligence Data Mining}
 \item{Kremers, Joost  - our main supervisor for guideline during the research }
 \item{Niels - Cyber Threat Intelligence project}
 \item{Ari Davies - proposed making the plugins also more depth in CTI }
 \item{Gijs Hollestelle - presented the actual CTI platform and the portal to it, proposed phishtank and cleared out the purpose for this project(making some agents for IR system) }
 \end{itemize}
\item{dhr. dr. C.P.J. (Karst) Koymans}
\end{itemize}
\newpage

\tableofcontents
\newpage

\section{Introduction}
%\addcontentsline{toc}{section}{Introduction}
\parbox{\linewidth}{
With the increasing number of cyber-attacks and the growth of computer crime worldwide, it becomes apparent that IT security is a major concern and crucial survival factor for large companies, organisations and institutions of any sort. Security Operations departments working to ensure confidentiality, integrity and availability for the system infrastructure of their organisation, invest huge  parts \cite{cyber} of their time and effort in detecting threats in real time. A very valuable source of security intelligence, vital to cyber-risk assessment, is information mined from data posted on public sites such as "pastebins" or social networks. However, this is a very cumbersome task due to the lack of Natural Language Processing capabilities in most of the existing tools. Moreover, as recent events have showcased \cite{sony}, several threats arise from governments and criminal associations originating from countries whose languages use non-latin scripts (Chinese, Russian, Korean, etc.). It is, hence, important to have data mining tools that provide support for such alphabets and languages, since a lot of a security intelligence can be discovered in such texts. The main goals of this research project will be to explore the various public data and detect the most appropriate among them. Moreover, numerous current data analytics techniques as well as their application on security related issues will be assessed. Lastly the above knowledge will be applied on the implementation of a simple system that will work as a proof-of-concept and help determine the technical feasibility, storage requirements and operational cost of such a system. This project was proposed by and will carried out in co-operation with Deloitte NL. 
}
\newpage
\subsection{Research Questions}
This topic is admittedly very open but it can be narrowed down to several specific research questions some of which we will try to answer to some extent. The main question that we will be trying to answer is the following:\\[0.1cm]

\noindent
\textbf{How can we effectively use public sources to obtain real time information about security incidents?}\\[0.1cm]

\noindent
This question can be analysed into more specific parts that cover the topic to some extent, as follows:

\begin{enumerate}
	\item How can the raw data be effectively collected from the public sources? 
	\begin{itemize}
		\item How can we effectively detect the reliable sources?
		\item What search terms can we deploy during the retrieval phase?
		\item How can the unstructured data be pre-processed? 
	\end{itemize}
	\item How can the data be analysed in respect to security operations?
	\begin{itemize}
		\item How can we apply current Data Mining and Analytics techniques on Security issues?
		\item How can we derive the risk assessment model from the above?
		\item How can we apply the model on new data?
	\end{itemize}
	\item	How can the collected knowledge be applied on a system implementation?
	\begin{itemize}
		\item What is a reliable and extensible System Architecture that can be designed?
		\item What are the computational and storage requirements of such a system?
		\item What extensions can be proposed for that system?
	\end{itemize}
\end{enumerate}

The proposed system extensions can spawn further research questions, namely on the topics of presenting the analysed data, reacting to the real time events and finally assessing the situations that arise and providing feedback to the system.


\subsection{Related work}
There is a lot of literature around the field of Data Mining and more recently Web Mining. The most prominent and recent case is the book 'Mining The Social Web' by M.Russel \cite{socialweb} that deals with exploring and mining information from social websites (e.g., Facebook, Twitter, LinkedIn, Google+, GitHub, etc.). There are also several academic papers and books that deal with applying Data Mining to System Security. One example is a system proposed by the university of Minnesota, called MINDS, that employs various Data Mining in Intrusion Detection. The system is described in their paper 'Data Mining for Cyber Security' \cite{minds}.  Another example is a system proposed by the Dutch company Sentient in co-operation with the Amsterdam Police Force \cite{police} aiming to provide Data Analytics operations automation while on the same time minimising the technical expertise needed by the system user. 
\subsection{Ethical implications}
%\addcontentsline{toc}{section}{Ethical implications}
The main part of this research comprises of exploring current techniques and their application on IT security, as well as the specification of a system that employs Data Mining techniques to collect security intelligence. In order for the models to be defined some amount of information will have to be gathered. This information will originate solely from public sources and will be mostly historical data. In the unlikely case that any previously unnoticed security issues are encountered they will be handled with discretion and communicated only towards the appropriate targets and only with the approval of the OS3 core team and Deloitte Digital. The collection or storage of personal data is not intended and any collected information will be discarded after the end of this project. The usage of shared computational and network infrastructure will only be used for the needs of this project and within the legal limits.

\newpage
\section{Methodology}
This project will be carried out with the aid of Deloitte Risk Services, Netherlands. The first step is to get acquainted with the Cyber-Risk Services and Data Analytics teams of the company and their people. When directed to the appropriate experts on each respective field (Security, Data Analytics, NLP, etc.) we will be working closely with them to collect the needed knowledge and expertise in order to reduce an effective risk assessment model. This will be made possible by applying Data Mining techniques that are already in use in other fields (economic risk assessment, marketing, etc.) on security issues. In parallel we will start designing and implementing the Information Retrieval part of the system. Once the appropriate amount of Data has been gathered from public sources we will apply the appropriate techniques that have been proposed to us on the collected dataset in order to design the risk assessment model. The next step will be to apply the designed model on new data and evaluate the effectiveness of the model. Lastly, based on the gathered experience and knowledge we will try to design a modular and extensible system architecture that deploys the designed techniques. Based on that design we will attempt to implement a simple proof of concept that will help us define the resource requirements and operational cost of such a system. 
\subsection{Installation requirements}
\parbox{\linewidth}{

Most of the work is going to be carried out on end workstations (desktops, laptops, etc.). Depending on the amount of data collected, additional computational or storage resources might be required. In that case our assigned OS3 servers can be used. As for software requirements, there are several open software tools (Database Systems, IR Systems, Web Crawlers, etc.) that can help us carry out the work. For each component of the system that has to be implemented manually the appropriate programming languages as well as libraries are going to be selected. One example is the very strong Python NLP-toolkit.\cite{nltk} }
\subsection{Proposed System}
 
\subsubsection{General Outline (picture)}
\subsubsection{Configuration}
\subsubsubsection{Specification}

It is fundamental to outline threat types and define the search keywords correctly as they compose the number of results that one will get from a database. Furthermore, commonly keywords should downgrade their importance when longer time measurements are taken since they will appear more often. The third important factor in defining the correct keywords and their weight is the volume of data being processed to filter out false positives and to get a good hit per ratio comparison. For example once we perform an initial security test of a company with the global keywords( e.g. sql injection, ddos attack), for each company we should make an instance of the agent, with company specific keywords, to add value to the monitored data and get the correct results for each company. Therefore, if we add to the aggregator module specific keywords they might give high performance on certain news highlights in a specific moment, but as we want to protect companies over time after clearing them of the most possible attacks such as sql injections to have a continuous monitoring of their system and to add value to our monitoring we should make a second monitoring instance that is more probed to their specific company needs. To construct and select the relevant keywords, previously discussed news \cite{list-2015-attacks} and messages related to specific threat can be analysed. 

Assuming that our target are all possible general attacks we can select the most common and popular \cite{owasp} and if we associate a list of keywords to some of this threats and apply a search method with the defined list, for a short period of time this can give some good results, once we stretch the time period we will get thousand of hits and this would be a bad selection of keywords.
\begin{center} 
\begin{longtable}{|l | l|}
\hline
Type of Attack & Related keywords \\
\hline
 Denial-of-Service (DoS) & ddos attack \\ 
 						& take down website \\ 
 						&  server/computer crush  \\
 						& server take down \\

\hline
SQL Injection 	& plain text password\\
				& clear text password \\
				& plain text password username \\
				& clear text password username\\
				& dump customers \\
				& dump passwords \\
				& blackmail dump accounts \\
				& leaked passwords \\
				
\hline
Account hijacking & account hacked \\
				  & account images changed hack \\
				  & take control account \\
				  & account add/remove content \\ 			  
\hline

\end{longtable}
\end{center}
\subsubsubsection{Implementation}
\subsubsection{Data Aggregation Module}
\parbox{\linewidth}{
\hfill \break
\\
This section describes a system for collecting and mining information from public sources. First subsection will define the relevant websites that provide interest in detecting threats, malwares and phishing sites. For each specific source, next subsections will investigate API functionality and steps in implementing crawling part.
}
\subsubsubsection{Specifications}
\parbox{\linewidth}{
\hfill \break
\\
Discovering real time threats with a plausible margin of error, became a challenging task as Internet provides an enormous database with random messages and information. Among these sites, significant information related to Security Intelligence can be found. Therefore it is important to define reliable sources that being monitored can provide significant valuable data for detecting future or present possible attacks. Special attention has to be provided in establishing which are the optimum sources that will provide the seeking information. 
\hfill \break
\\
What sources can be considered as reliable? In order to answer this question inside research has been given by experts. From a company perspective it is important to have an overview over all general possible attacks or client specific. Among all the possible feed sources, four are considered at this moment relevant for collecting security related data. Other sources that provide RSS feeds or IRC channels will not be investigated due to time limitations and company requirements. This research will primarily focus on the following sites: Phishtank, Pastebin, Twitter and Reddit. Mainly, these are considered fundamental for the reason that past activity revealed vulnerabilities found through posted messages.  Furthermore, mentioned sources will be analysed in particular and be judged by the relevance of the subjects involved in discussion. 
}
\subsubsubsection{Implementation}
\paragraph*{Twitter}
\addcontentsline{toc}{paragraph}{Twitter}
\newpage
\paragraph*{Pastebin}
\addcontentsline{toc}{paragraph}{Pastebin}
\hfill \break
\\
\textit{Pastebins} websites \cite{fpaste} \cite{pastebin} allow everyone with or without registration to share real time text or code snippets. It attracted many users during past years including cybercriminals like malware writers\cite{pastebin-magazine}. The enormous flow of information include from database dumps containing e-mails and passwords to harmful backdoor programs. At a deeper examination of these public pasted messages possible future attacks can be determined. From all possible \textit{pastebins} sources (e.g. fpaste.org \cite{fpaste}, paste2.org, pastie.org\cite{pastebin-pastie}, 
paste.ubuntu.org.cn \cite{pastebin-ubuntu} etc.) pastebin.com \cite{pastebin} will be further inspected and monitored.
\hfill \break
\\
Pastebin API offers different options for developers, more specifically, creating new pastes, listing trending pastes and pastes from a particular user. This extensions can be used once one obtains the unique Developer API Key. 
\hfill \break
\\
At this moment Pastebin API \cite{pastebin} does not offer complete information retrieval option for listing all new messages that arrive from unregistered users. The only option available now, would be to list messages from registered users, however all information provides interest. This makes the crawling module challenging but not impossible. A method will be introduced to surpass this limitation. 
\hfill \break
\\
Search method for retrieving information using combination of keywords, from the \textit{pastebin} website, uses third party software namely, \textit{Google Custom Search}. Therefore, if one desires to obtain full database related to the keywords specified, it would have to use Google's API. Free users account will be limited to 1000 pastes per day. 
\hfill \break
\\
To overcome API extension limitation and mine all new pastes that arrive in real time a web crawler module was implemented. Python offers an open source framework called \textit{scrapy} \cite{scrapy} for building an extensible webcrawler. Pastebin's \texttt{Archive} page contains latest pastes posted in the last 10 minutes at least. We will crawl and fetch this page each 10 minutes and store data into mongoDB database. Not all data from the page provides interest this is why we will scrape only information that is relevant for further examination. This includes the url, paste, the date it was posted and the number of unique views. By inspecting the elements required from the DOM tree, we then can parse this nodes and extract only the text. Once all this data has been gathered, it will be dumped in the database.
\paragraph*{Phishtank}
\newpage
\paragraph*{Reddit}
\addcontentsline{toc}{paragraph}{Reddit}
\hfill \break
\\
Reddit community comprises social websites and news sites. It contains over 5000 channels called "subreddits" which belong to different categories. This research will mine Reddit \cite{reddit} data from the following branches:
\begin{itemize}
\item /r/blackhat \cite{red-blackhat}
\item r/malware \cite{r.malware}
\item r/netsec \cite{r.netsec}
\item r/pwned \cite{r.pwned}
\item /r/vrd/ \cite{r.rvd}

\end{itemize}
\hfill \break
\\
Reddit API\cite{reddit} provides a rich list of methods which are easy to use. For listing new, top, controversial etc. messages is sufficient to add "/" to the subreddit one wants to obtain information. Endpoints can be used to filter and sort posts namely, \textit{after/before, limit, count} and \textit{show} \cite{reddit}. This parameters will be later used in retrieving lists of messages in json format. 
\hfill \break
\\
All ingredients for implementing a retrieval agent for Reddit are supplied by its API. The only part we need to do is to filter and obtain data that further investigated and processed can help defining the relevance of this information. First a connection needs to be opened to the the targeted subreddit, in this case to /r/blackhat. 

\begin{spverbatim}
response = urllib2.urlopen('http://www.reddit.com/r/blackhat/new
			.json?sort=new&limit=100')
\end{spverbatim}
\hfill \break
Response variable will retrieve a file in json format. From the provided file we will store the title associated to the url, number of comments and score, because this information provides interest for further processing. It is important to check the date to a corresponding message, as a message posted that has many votes but was posted long time ago might get a higher score in comparison to one recently posted and having lower score. 
\hfill \break
\\
Python provides an open json library for processing this type of files. All scraped Reddit data will be stored in mongoDB database. 

\paragraph*{Trigger System}
\addcontentsline{toc}{paragraph}{Trigger System}
\subsubsection{Deflation/Warehousing/Filtering}
\subsubsubsection{Specification}
\subsubsubsection{Implementation}
\subsubsubsection{Alternatives}
An aggressive method for deflating documents has been previously introduced. This subsection will outline as alternatives known processes for cleaning documents by removing stop words, stemming and recognizing part of speech with a further extension to TF-IDF alghorithm for determining the relevance and weights for the documents.
\paragraph*{NLTK}
NLTK or Natural Language Toolkit ...

\paragraph*{TF-IFD}
\addcontentsline{toc}{paragraph}{TF-IFD}
\hfill \break
\\
TF-IDF \cite{tf-idf} stands for term frequency–inverse document frequency and it is used to statistically score the importance of a word in a document or across a collection of documents. A word will be considered important if it appears multiple times into a document. TF is calculated by counting how many times the specific word appears, over the total number of words from that document and it will get a high score. But if it appears and in other documents it will not be as unique and will receive a lower score. 
\\
\\
%Having this in mind we implemented and applied the algorithm  on the retrieved data but the results turned out to be not sufficient to grade the value of the documents.
To determine the most relevant documents related to given keywords, first is required to  define query terms which are related to the known threats categories. These will be further processed by the algorithm and as a result a statistical list of the most significant documents will be obtained. This is an automated method for sorting document that are more appropriate to a specific queried subject, given on-topic keywords list.
\newpage
\subsubsection{Analytics and Mining}
This section will sketch few methods that can be applied over the congregated information. All the unstructured data has to be processed and converted to "clean" structured information which includes natural language processing. For this step Python provides an open source library nltk \cite{nltk}. We have to dip into several phases to get a good transformation of  paste, mesages or titles. This includes stop words removal, stemming and pos(part of speach) detection. 
At this moment there are various data mining techniques \cite{oracle-list} that include text processing. Investigating all this techniques is out of scope in this project, therefore the once that are mostly commonly used will be provided. In data mining step or knowledge discovery from database(KDD) it is important to find and extract patterns and knowledge from database not the data itself \cite{data-kdd}.
\subsubsubsection{Specification}
\subsubsubsection{Implementation}
\subsubsection{Feedback and Assessment}
\subsubsubsection{Specification}
\paragraph{Data mining tasks}
\begin{itemize}
\item 
Classification: Refers to the task of generalising known structures and applying them to new
data. For example, an e-mail program might attempt to classify an e-mail as "legitimate" or as
“spam”. Since this involves training of the algorithm before use, it might not be ideal for use in
threat discovery. However, it can be very useful for providing feedback to the system and fine
tuning it to be able to classify the documents for each threat category, acting together with the
configuration step
\item 
Anomaly detection (Outlier/change/deviation detection): This task refers to identification of
unusual data records that might imply unusual behaviour in the dataset. For example, in our
use case, a first indication that something might be possibly wrong would be an exceptional
mention of a company name within a certain period of time. The problem here is to determine
what can be defined as exceptional. This of course is not possible without the use of old data
so a certain threshold can be determined. Until sufficient amount of data has been gathered we
can naively implement this by setting the thresholds manually. This operation is particularly
popular in intrusion detection systems.
\item 
Association rule learning (Dependency modelling): This task refers to the search for
relationships between variables. One example of this is the infamous beer and diapers story.
This test requires no training set and can be performed on any structured set of data. In our
use case this can help us determine trends between the occurrences of keywords, such as the
name of a company together with the name of a name of an attack. The output of this test also
contains metrics about the support (% of the documents supporting the association) as well as
confidence (ratio of the support percentage of either part of each rule).
\item 
Clustering: Refers to the task of discovering groups and structures in the data that present
some sort of similarity, without using known structures in the data. In our case this can help us
determine new categories of documents that have not been predefined (remember the threat
categorisation we mentioned in the beginning).
\item
Regression: This task attempts to find a function which models the data with the least error.
This might not be directly applicable to our specific example but based on the experience
gathered after a certain period of time that the system will be operated, we can attempt to
design an analytic model using regression analysis techniques.
\item
Summarisation: This task aims to provide a more compact representation of the data set,
including visualisation and report generation. In our case this applies to the creation of the
alerts to be presented to the Security Analysts. The alerting will consist of a common format
that will apply to automatically generated events that will be propagated to the appropriate
parties. One proposed format could be the following:
\{alert\_id: 00001, subject: "Something is rotten in the state of Denmark", importance: Red,
%backing_documents:\[1,3,4,6\]\}
This presents the analyst with an event informing them about a security incident with a certain
importance level. The way of determining the level of importance falls out of the scope of this
document. Notice also the list of supporting documents resulting from different sources. At this
point the analyst can go and inspect each one of this documents manually, assess the risk
themselves and take the appropriate measures such as further informing the involved parties, or
ignoring the alert.
\item Attribute Important 
\item etc.
\end{itemize}
\subsubsubsection{Implementation}
\paragraph*{Association Rule}
\addcontentsline{toc}{paragraph}{Association Rule}
\paragraph*{Clustering}
\addcontentsline{toc}{paragraph}{Clustering}
\hfill \break 
\\
Clustering task is important in automating the process of finding new type of attacks or vulnerabilities that might occur in a specific period of time. The proposed method will categorize related documents that belong to a certain cluster and the most frequent related keywords associated to the specific cluster.
\\
\\
For clustering N collected documents from the database in K categories an option would be using K-means\cite{k-means} algorithm, also known as Lloyd's algorithm. The method\cite{k-means-example} proposed by this algorithm aims to find K non-overlapping clusters and it works as follows. User specifies how many K clusters will represent the initial centroids, chosen randomly, in which the information will be divided into. Assign each observation point(word) to the closest centroid, where each collection of points(words) form a cluster. The centroid is then updated depending of the words added to this cluster. The process stops when the words assigned will not change the clusters. At this moment there is no guarantee that the result retrieved is optimum. The outcome might be different each time, depending on how the initial centroids are chosen. Important factor to mention here is the selection of K, because an incorrect choice might lead to non-optimum results. There are several procedures\cite{procedures-for-kmeans} to calculate a favourable number of clusters but this is out of scope and the investigation, and we will consider random cluster number that will be feasible chosen compared to the amount of data.
\\
\\
In order to implement the proposed algorithm an open  source scikit- learn \cite{sklearn} tool was used for data analysing that provides a library called SKlearn which can be integrated in Python environment. Following an example provided in a tutorial from the sklearn website, which attemps to categorize 20 news group\cite{k-means-20news}, we adapted it to use the data provided in the database and show what are the documents that are related to each other in a cluster.  In this case we applied the algorithm on over 600000 twitter messages and the expected outcome was to categories this messages in 20 groups. From the clusters obtained two showed big interest as it presents real events that happened in the past weeks.
\begin{spverbatim}
Cluster 13:  paris  charlie  hebdo  attack  http  mayor  nypd  
rt  victims  french  honor  visited  nyc  france  terror  muslim 
 bolsters  security  jewish  cover
Cluster 15:  photos  leaked  upton  kate  jennifer  lawrence 
 nude  victoria  justice  megan  fox  http  seen  rt  hoeuu2dubr  
itweetlikegirls  hilarious  kardashian  kim  aigbtbpmvv
\end{spverbatim}
\hfill \break
Observing the resulted clusters we can claim that the algorithm behaved well considering that it is an unsupervised method and the top frequent terms for Cluster 13, highlights the most recent attack in Paris. In the Cluster 15 which is totally unrelated to the previously mentioned attack, the incident with leaked celebrities photos is outlined.
\paragraph*{Classification}
SVM Algorithm 

\section{Results}
\subsection{Demo}
\newpage
\section{Conclusions}
\newpage
\section{Future Work}

At this moment we have implemented a modular and pipelined system by using open source tools. As a future extension, one target would be to have a full system implementation. This system can be further integrated with already developed CTI (Cyber Threat Intelligence) portal. Due to the lack of time, we have build a simple training set in the classification module, just to showcase that it is a feasible attempt to categorize a new given set of items based on the previous trained set. A normal step would be to build numerous training sets from the real world data, gathered from social websites, and test them on the new incoming data. This step requires time as new attacks might happen in a week, month or year. Therefore at least one year would be necessary to have a stable, favourable trained set. Classification method can also be used for exploring sentiment analyses for the collected messages. To build a set that will distinguish false positives, at first this can be done manually by a Security Analyst which could split a real attack from just a random message. Moreover, natural language processing capabilities can be explored to filter messages from all the unnecessary data and retrieve only the interesting data that can be further structured for a further analyse. Clustering method that we proposed for finding new categories of attacks that might have been omitted, uses a random K number defined by user. For optimised results, a good solution would be to check the proposed methods for finding the optimal number of clusters in a given dataset. An expected outcome for the assessment module would be the ability to rise real time alerts in the moment a threat or attack to certain company or client is found.

\newpage
\section{Appendix}
\newpage

\begin{thebibliography}{99}
\bibitem{cyber}
US cybercrime: Rising risks, reduced readiness, PWC, Available at: \url{http://www.pwc.com/en_US/us/increasing-it-effectiveness/publications/assets/2014-us-state-of-cybercrime.pdf}
\bibitem{sony}
	 D. Sanger, N. Perlroth.  U.S. Said to Find North Korea Ordered Cyberattack on Sony. [online] Nytimes.com. \\Available at: \url{http://www.nytimes.com/2014/12/18/world/asia/us-links-north-korea-to-sony-hacking.html?_r=0}
\bibitem{socialweb}
  Russell, MA (2014). Mining the Social Web, O'Reily Media, USA
\bibitem{minds}
    V. Chandola et al. Data Mining for Cyber Security, Department of Computer Science, University of Minnesota, Springer, 2006
\bibitem{police}
   RCP van der Veer, H.T. Roo,  A. van der Zanden, Data mining for intelligence led policing, Sentient, Amsterdam Police Force, Amsterdam, The Netherlands, 2009
\bibitem{nltk}
Nltk.org, (2015). Natural Language Toolkit - NLTK 3.0 documentation. [online] Available at: \url{http://www.nltk.org/}

\bibitem{fpaste}
Fedora Project Pastebin Fpaste.org, (2015). New paste • Fedora Project Pastebin. [online] Available at:  
\url{http://fpaste.org }
\bibitem{pastebin}
Pastebin, (2015). Pastebin.com - \#1 paste tool since 2002!. [online] Available at:     \url{http://pastebin.com/}

\bibitem{pastebin-magazine}
Security Intelligence, (2015). Pastebin a Convenient Way for Cybercriminals to Remotely Host Malware. [online] Available at: \url{http://securityintelligence.com/news/pastebin-convenient-way-cybercriminals-remotely-host-malware/#.VMS25DX8vCI }
\bibitem{pastebin-pastie}
Pastie.org, (2015). New - Pastie. [online] Available at: \url{http://pastie.org} 
\bibitem{pastebin-ubuntu}
 Paste.ubuntu.org.cn, (2015). [online] Available at: \url{http://paste.ubuntu.org.cn/}
\bibitem{scrapy}
Scrapy.org, (2015). Scrapy | A Fast and Powerful Scraping and Web Crawling Framework. [online] Available at: \url{http://scrapy.org/} 
\bibitem{reddit}
Reddit.com, (2015). reddit: the front page of the internet. [online] Available at: \url{http://www.reddit.com/} 
\bibitem{red-blackhat}
Reddit, (2009). blackhat library: Documenting blackhat hacking techniques • /r/blackhat. [online] Available at: \url{https://www.reddit.com/r/blackhat}
\bibitem{r.malware}
Reddit, (2009). Malware Analysis \& Reports • /r/Malware. [online] Available at:   \url{http://www.reddit.com/r/malware}
\bibitem{r.netsec}
reddit, (2007). /r/netsec - Information Security News \& Discussion. [online] Available at: \url{https://www.reddit.com/r/netsec/} 
\bibitem{r.pwned}
Reddit, (2008). pwned • /r/pwned. [online] Available at: http://www.reddit.com/r/pwned 
\bibitem{r.rvd}
Reddit, (2012). Vulnerability Research and Development • /r/vrd. [online] Available at: \url{http://www.reddit.com/r/vrd/}
\bibitem{oracle-list}
Oracle.com, (2015). Oracle Data Mining Techniques and Algorithms. [online] Available at: \url{http://www.oracle.com/technetwork/database/enterprise-edition/odm-techniques-algorithms-097163.html}
\bibitem{tf-idf}
Tfidf.com, (2015). Tf-idf : A Single-Page Tutorial - Information Retrieval and Text Mining. [online] Available at: \url{http://www.tfidf.com/} 
\bibitem{data-kdd}
Han, Jiawei; Kamber, Micheline (2001). Data mining: concepts and techniques. Morgan Kaufmann. p. 5. ISBN 9781558604896. "Thus, data mining should have been more appropriately named "knowledge mining from data," which is unfortunately somewhat long"
\bibitem{owasp}
Owasp.org, (2015). Category:OWASP Top Ten Project - OWASP. [online] Available at: \url{https://www.owasp.org/index.php/Top10#OWASP_Top_10_for_2013} 
\bibitem{list-2015-attacks}
Passeri, P., Passeri, P. and Passeri, P. (2015). Cyber Attacks Timeline | Hackmageddon.com. [online] Hackmageddon.com. Available at: http://hackmageddon.com/category/security/cyber-attacks-timeline/ 
\bibitem{sklearn}
Scikit-learn.org, (2015). scikit-learn: machine learning in Python — scikit-learn 0.15.2 documentation. [online] Available at: \url{http://scikit-learn.org/stable/index.html}
\bibitem{orange}
Bioinformatics Laboratory, U. (2015). Orange Data Mining. [online] Orange.biolab.si. Available at: \url{http://orange.biolab.si/} 
\bibitem{k-means} Junjie Wu , Advances in K-means Clustering: A Data Mining Thinking
\bibitem{k-means-example}
Gonçalves, H. (2015). K-means clustering - algorithm and examples. [online] Onmyphd.com. Available at: \url{http://www.onmyphd.com/?p=k-means.clustering}
\bibitem{procedures-for-kmeans}    Glenn W. Milligan, Martha C. Cooper , An examination of procedures for determining the number of clusters in a data set
\bibitem{k-means-20news}
Scikit-learn.org, (2015). Clustering text documents using k-means — scikit-learn 0.15.2 documentation. [online] Available at: \url{http://scikit-learn.org/stable/auto_examples/document_clustering.html} 
 
\end{thebibliography}
\end{document}